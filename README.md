# ğŸ¤– QnA Application with OpenAI and Ollama

An interactive question-answering chatbot app that leverages OpenAI and Ollama models using the Langchain framework â€” deployed via Streamlit. Built to compare the performance and user experience of commercial and open-source LLMs in a practical use case.

## ğŸ“Œ Features
* Supports both OpenAI GPT models and open-source LLMs via Ollama
* Real-time Q&A interaction in a Streamlit interface
* Adjustable model settings: temperature, max tokens, and model selection
* Modular, easy-to-extend codebase using Langchain components

## ğŸ§  Technologies Used
* Streamlit
* Langchain
* OpenAI API
* Ollama (open-source LLM framework)
* Python dotenv
* Prompt Engineering with ChatPromptTemplate

## ğŸŒ Project Structure
```
â”œâ”€â”€ app.py           # Streamlit app using OpenAI GPT models
â”œâ”€â”€ ollama.py        # Streamlit app using Ollama (open-source LLMs)
â”œâ”€â”€ requirements.txt # All dependencies for environment setup
â””â”€â”€ .env             # (You must create this)
```

## âš™ï¸ Environment Setup
1. Clone the Repo
```
git clone https://github.com/bhaskar2311/QnA-Application-with-OpenAI-and-Ollama
cd QnA-Application-with-OpenAI-and-Ollama
```
2. Create a Virtual Environment (optional but recommended)
```
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```
3. Install Dependencies
```
pip install -r requirements.txt
```
4. Setup Environment Variables
  * Create a .env file in the root directory and add your API keys like this:
```
# .env
OPENAI_API_KEY=your_openai_api_key_here
LANGSMITH_API_KEY=your_langsmith_key_here
```
ğŸ” Note: Never share your .env file publicly â€” it contains sensitive keys.

## ğŸš€ How to Run
#### ğŸ§  Using OpenAI (app.py)
```
streamlit run app.py
```
* Input your OpenAI API key in the sidebar
* Choose model (e.g., gpt-4o)
* Adjust temperature and max tokens
* Ask your question and view responses in real-time

#### ğŸ¦™ Using Ollama (ollama.py)
```
streamlit run ollama.py
```
* Select the open-source LLM (e.g., llama3.1)
* Adjust parameters like temperature and token length
* Ask questions â€” powered by Ollama


## ğŸ“ Detailed Logic
#### ğŸ”¹ app.py â€“ OpenAI Chatbot
* Loads .env using load_dotenv()
* Reads API key from sidebar input
* Sets LangSmith tracking with project metadata
* Uses ChatOpenAI model from Langchain
* Chains prompt â†’ model â†’ output parser
* Supports configurable temperature and max_tokens from UI
* Displays real-time answers in a friendly UI

#### ğŸ”¹ ollama.py â€“ Ollama Chatbot
* Similar structure to app.py
* Uses Ollama model via langchain_community.llms
* Requires no API key (for local open-source models)
* Reads model and parameters from Streamlit sidebar
* Uses the same ChatPromptTemplate and StrOutputParser
* Ideal for testing LLMs like llama3.1

## ğŸ“¦ Reusability
* The requirements.txt is designed for reuse across all LLM projects.
* Only .env setup and model choice will vary.

## ğŸ“¸ Screenshots
#### With OpenAI
![Output of OPEN AI Project](https://github.com/user-attachments/assets/c99165be-9e19-4ebb-a168-0ba7382fd140)

#### With Ollama
![Output of Ollama AI Project](https://github.com/user-attachments/assets/11cf9021-678a-4910-be31-7638664cb106)

## ğŸ“ Notes
This project was fully developed by me. The README was written based on my knowledge and experience, with support from generative AI tools to refine its structure and presentation.

## ğŸ“„ License
This project is open source and available under the MIT License.

## ğŸ™‹â€â™‚ï¸ Author
Made with â¤ï¸ by Bhaskar Shivaji Kumbhar









